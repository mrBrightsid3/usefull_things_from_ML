{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basepath = 'aclImdb'\\n\\nlabels = {'pos' : 1, 'neg' : 0}\\npbar = pyprind.ProgBar(50000)\\ndf = pd.DataFrame()\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\"\"\"basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos' : 1, 'neg' : 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for s in ('test', 'train'):\\n    for l in ('pos', 'neg'):\\n        path = os.path.join(basepath, s, l)\\n        for file in sorted(os.listdir(path)):\\n            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\\n                txt = infile.read()\\n            df = df.append([[txt, labels[l]]], ignore_index=True)\\n            pbar.update()\\n\\ndf.columns = ['review', 'sentiment']\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "\n",
    "df.columns = ['review', 'sentiment']\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df = df.reindex(np.random.permutation(df.index))\\ndf.to_csv('movie_data.csv', index=False, encoding='utf-8')\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\"\"\"df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 6,\n",
       " 'sun': 4,\n",
       " 'is': 1,\n",
       " 'shining': 3,\n",
       " 'weather': 8,\n",
       " 'sweet': 5,\n",
       " 'and': 0,\n",
       " 'one': 2,\n",
       " 'two': 7}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()#создаем 1-граммы, чтобы создать 2-граммы передаем ngram_range = (2, 2)\n",
    "docs = np.array(['The sun is shining',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)\n",
    "count.vocabulary_#глосарий - слово и индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the sun': 9,\n",
       " 'sun is': 7,\n",
       " 'is shining': 1,\n",
       " 'the weather': 10,\n",
       " 'weather is': 11,\n",
       " 'is sweet': 2,\n",
       " 'shining the': 6,\n",
       " 'sweet and': 8,\n",
       " 'and one': 0,\n",
       " 'one and': 4,\n",
       " 'one is': 5,\n",
       " 'is two': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(ngram_range = (2, 2))\n",
    "docs = np.array(['The sun is shining',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)\n",
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.toarray()#вектора признаков, сырые частоты термов \n",
    "#tf(t, d) - сколько раз терм t встречается в документе d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#мера частота терма - обратная частота документа позволяет найти часто встречающиеся слова\n",
    "#которые не имеют смысла tf-idf (t, d) = tf(t, d) * idf(t, d)\n",
    "#idf(t, d) = log(nd/(1 + df(t, d))) nd - общее количество документов\n",
    "#df(t, d) - количество документов d содержащих терм t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.58 0.   0.   0.   0.   0.   0.58 0.   0.58 0.   0.  ]\n",
      " [0.   0.   0.58 0.   0.   0.   0.   0.   0.   0.   0.58 0.58]\n",
      " [0.57 0.22 0.22 0.28 0.28 0.28 0.28 0.22 0.28 0.22 0.22 0.22]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True,\n",
    "                         norm='l2',#нормализуем вектор термов деля на L2 норму\n",
    "                         smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#в sklearn формулы отличаются, а именно:\n",
    "#idf(t, d) = log(1 + nd/(1 + df(t, d)))\n",
    "#tf-idf (t, d) = tf(t, d) * (idf(t, d) + 1) из-за того что smooth_idf=True чтобы  = 0 для термов встречающихся во всех документах\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review'][-50:] #данные необходимо очитстить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) #удаление html разметки\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:-\\)|\\(|D|Р)',\n",
    "                           text)#находми все эмотиконы\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + #находим и удалям все несловарные символы\n",
    "            ' '.join(emoticons).replace('-', ''))#возвращаем эмотиконы\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven title brazil not available'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text):#один из вариантов разбиения на лексемы\n",
    "    return(text.split())\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#аналог - стемминг - преобразование слова в его корневую форму\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'thu', 'run']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#удаление стоп-слов - слова, которые распространены в тексте и вероятно не несут в себе полезной информации\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('runners like running and thus they run')[-10:]\n",
    " if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.model_selection import GridSearchCV\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\ntfidf = TfidfVectorizer(strip_accents=None,\\n                        lowercase=False,\\n                        preprocessor=None)\\n\\nparam_grid = [{'vect__ngram_range': [(1,1)],\\n               'vect__stop_words': [stop, None],\\n               'vect__tokenizer': [tokenizer,\\n                                  tokenizer_porter],\\n               'clf__penalty': ['ll', 'l2'],\\n               'clf__C': [1.0, 10.0, 100.0]},\\n              {'vect__ngram_range': [(1,1)],\\n               'vect__stop_words': [stop, None],\\n               'vect__tokenizer': [tokenizer,\\n                                    tokenizer_porter],\\n               'vect__use_idf': [False],\\n               'vect__norm' : [None],\\n               'clf__penalty': ['11', '12'],\\n               'clf__C': [1.0, 10.0, 100.0]}]\\n\\nlr_tfidf = Pipeline([('vect', tfidf),\\n                     ('clf',\\n                      LogisticRegression(random_state=0,\\n                                         solver='liblinear'))])\\ngs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\\n                           scoring='accuracy',\\n                           cv=5, verbose=2,\\n                           n_jobs=-1)\\ngs_lr_tfidf.fit(X_train, y_train)\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1,1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer,\n",
    "                                  tokenizer_porter],\n",
    "               'clf__penalty': ['ll', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1,1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer,\n",
    "                                    tokenizer_porter],\n",
    "               'vect__use_idf': [False],\n",
    "               'vect__norm' : [None],\n",
    "               'clf__penalty': ['11', '12'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]}]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf',\n",
    "                      LogisticRegression(random_state=0,\n",
    "                                         solver='liblinear'))])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5, verbose=2,\n",
    "                           n_jobs=-1)\n",
    "gs_lr_tfidf.fit(X_train, y_train)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#динамическое обучение\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text) #удаление html разметки\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:-\\)|\\(|D|Р)',\n",
    "                           text)#находми все эмотиконы\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + #находим и удалям все несловарные символы\n",
    "            ' '.join(emoticons).replace('-', ''))#возвращаем эмотиконы\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding = 'utf-8') as csv:\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,#большое кол-во признаков - меньше шанс коллизии \n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log', random_state=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VIKTOR\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:27\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "#обучаем на 45000 данных из набора\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8682\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "#проверяем на следующих 5000\n",
    "X_test = vect.transform(X_test)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)#дообучаем на тех данных на которых проверяли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#современная модель суммирования слов - word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Тематическое моделирование - тема для непоименнованного документа\n",
    "#латентное размещение Дирихле (LDA) - отыскивает группы слов часто появляющихся вместе\n",
    "#раскладывает матрицу суммирования слов на матрицу отображения документов на темы\n",
    "#и матрицу отображения слов на темы при перемножении они дадут матрицу суммирования слов\n",
    "#результат - темы в матрице суммирования слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,#максимальная частота слов подлежащих рассмотрению избежать (is, and, a, the)\n",
    "                        max_features=5000)#ограничение на размерность данных\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch',#за 1 раз все, аналог online (динамическое обучение)\n",
    "                                n_jobs=-1)\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape #матрица значения важности слов (5000) для каждой из 10 тем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тема №1\n",
      "worst minutes awful script stupid\n",
      "Тема №2\n",
      "family mother father children kids\n",
      "Тема №3\n",
      "american war dvd music black\n",
      "Тема №4\n",
      "human audience art cinema feel\n",
      "Тема №5\n",
      "police guy car dead murder\n",
      "Тема №6\n",
      "horror house sex blood woman\n",
      "Тема №7\n",
      "role performance comedy actor performances\n",
      "Тема №8\n",
      "series episode episodes tv season\n",
      "Тема №9\n",
      "book version original effects fi\n",
      "Тема №10\n",
      "action fight guy guys cool\n"
     ]
    }
   ],
   "source": [
    "#выведем 5 самых важных слов для каждой темы (в матрице в порядке возрастания)\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Тема №{topic_idx + 1}')\n",
    "    print(' '.join([feature_names[i]\n",
    "                    for i in topic.argsort()\n",
    "                        [:-n_top_words - 1: -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#в целом плохие фильмы\n",
    "#Семейные фильмы\n",
    "#Военные фильмы\n",
    "#фильмы об искусстве\n",
    "#криминальные фильмы\n",
    "#фильмы ужасов\n",
    "#комедийные фильмы\n",
    "#фильмы связанные с ТВ шоу\n",
    "#по мотивам книг\n",
    "#фильмы боевики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Фильм ужасов №1\n",
      "<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ...\n",
      "\n",
      "Фильм ужасов №2\n",
      "Before I talk about the ending of this film I will talk about the plot. Some dude named Gerald breaks his engagement to Kitty and runs off to Craven Castle in Scotland. After several months Kitty and her aunt venture off to Scottland. Arriving at Craven Castle Kitty finds that Gerald has aged and he ...\n",
      "\n",
      "Фильм ужасов №3\n",
      "This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n"
     ]
    }
   ],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print(f'\\nФильм ужасов №{iter_idx + 1}')\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сохраняем последнюю модель\n",
    "import pickle\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "pickle.dump(stop,\n",
    "            open(os.path.join(dest, 'stopwords.pkl'), 'wb'),#запись в двоичном режиме\n",
    "            protocol=4)\n",
    "pickle.dump(clf,\n",
    "            open(os.path.join(dest, 'classifier.pkl'), 'wb'),\n",
    "            protocol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
